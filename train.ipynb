{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7247372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353395ee",
   "metadata": {},
   "source": [
    "##### GPT-2 - 124M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13cbf226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d812f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "for k,v in model.state_dict().items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7600ec66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124439808"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8db02969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4579,  0.1656, -0.7620,  0.3053,  0.0652, -0.1108,  0.9210,  0.4327,\n",
       "        -0.0926,  0.6806, -1.2805,  0.3362])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,4)\n",
    "\n",
    "a.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2324c416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32d523cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello Boss! Welcome to my new account, Boss. I'll be your new best friend, your new best buddy. I'll be your new best friend, your new\"},\n",
       " {'generated_text': 'Hello Boss! Welcome to the second episode of our episode of the \"Bravo Guide to the Superheroes\" series, and it\\'s a very special episode.'},\n",
       " {'generated_text': 'Hello Boss! Welcome to the Dungeon! Welcome to the World! Welcome to the Wolfpack! Welcome to the Underdog Bounty Hunter Welcome to the Underdog Bounty Hunter 2'},\n",
       " {'generated_text': \"Hello Boss! Welcome to the new book, The Magician's Apprentice: The True Story of the Mystic Thief. It gives you a comprehensive look at the development of the\"},\n",
       " {'generated_text': \"Hello Boss! Welcome to Smash Mouth.\\n\\nYou can follow Smash Mouth on Twitter at @Smash_Mew, or if you're not sure of where to\"}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "set_seed(42)\n",
    "pipe(\"Hello Boss! Welcome\", max_new_tokens=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b5eaff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from dataclasses import dataclass\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        \n",
    "        # Key, Query, Value projections combined\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "        # Causal mask\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch, sequence length, embedding dim\n",
    "        \n",
    "        # Calculate Q, K, V\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # (B, nh, T, hd)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / (self.head_dim ** 0.5))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        y = att @ v  # (B, nh, T, hd)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.act = nn.GELU(approximate=\"tanh\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd)\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Sequence length {T} exceeds block size {self.config.block_size}\"\n",
    "        \n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0)\n",
    "        \n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name):\n",
    "        \"\"\"Load pretrained GPT-2 weights from HuggingFace\"\"\"\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        \n",
    "        print(f\"Loading weights from pretrained GPT-2: {model_name}\")\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        \n",
    "        config = GPTConfig(\n",
    "            block_size=model_hf.config.n_positions,\n",
    "            vocab_size=model_hf.config.vocab_size,\n",
    "            n_layer=model_hf.config.n_layer,\n",
    "            n_head=model_hf.config.n_head,\n",
    "            n_embd=model_hf.config.n_embd\n",
    "        )\n",
    "        \n",
    "        gpt = cls(config)\n",
    "        sd = gpt.state_dict()\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        \n",
    "        # Map HuggingFace keys to our keys (they use Conv1D, we use Linear)\n",
    "        sd_keys_hf = [k for k in sd_hf.keys() if not k.endswith('.attn.masked_bias') and not k.endswith('.attn.bias')]\n",
    "        \n",
    "        # HuggingFace uses Conv1D which stores weights transposed\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        \n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # Transpose Conv1D weights to Linear weights\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape, f\"Shape mismatch for {k}\"\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # Direct copy\n",
    "                assert sd_hf[k].shape == sd[k].shape, f\"Shape mismatch for {k}\"\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "        \n",
    "        print(f\"Loaded {len(sd_keys_hf)} weight tensors\")\n",
    "        return gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "eec9bbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from pretrained GPT-2: gpt2\n",
      "Loaded 149 weight tensors\n"
     ]
    }
   ],
   "source": [
    "# Load the model with properly mapped weights\n",
    "mymodel = GPT.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "98dcec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Hello Boss! Welcome\n",
      "Generated: Hello Boss! Welcome Frenker FTL: Faster Than Light Fuego! Fugl Full Bore Full Metal Furies Full Mojo Rampage Full Throttle Remastered Full\n"
     ]
    }
   ],
   "source": [
    "def generate(model, prompt, max_new_tokens=50, temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    Generate text using the GPT model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    input_ids = torch.tensor([tokens])\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop input if it exceeds block size\n",
    "            idx_cond = input_ids if input_ids.size(1) <= model.config.block_size else input_ids[:, -model.config.block_size:]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(idx_cond)\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Optional top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    \n",
    "    # Decode and return\n",
    "    generated_tokens = input_ids[0].tolist()\n",
    "    return tokenizer.decode(generated_tokens)\n",
    "\n",
    "\n",
    "# Example usage - now with properly loaded weights!\n",
    "prompt = \"Hello Boss! Welcome\"\n",
    "generated_text = generate(mymodel, prompt, max_new_tokens=30)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fbad3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-task (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
